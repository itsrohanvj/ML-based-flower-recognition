{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I'll train an image classifier to recognize different species of flowers. We can imagine using something like this in a phone app that tells you the name of the flower your camera is looking at. In practice you'd train this classifier, then export it for use in your application. We'll be using the flower dataset (http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) of 102 flower categories.\n",
    "\n",
    "The project is broken down into multiple steps:\n",
    "\n",
    "* Load and preprocess the image dataset\n",
    "* Train the image classifier on your dataset\n",
    "* Use the trained classifier to predict image content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6e05b2f9003e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "# Imports here\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sb\n",
    "#import workspace_utils\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from collections import OrderedDict\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here 'torchvision' is used to load the data.(http://pytorch.org/docs/0.3.0/torchvision/index.html)).                  The dataset is split into three parts, training, validation, and testing. For the training, we'll apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. We'll also make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n",
    "\n",
    "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this any scaling or rotation transformations are not needed, but we'll need to resize then crop the images to the appropriate size.\n",
    "\n",
    "The pre-trained networks we'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets we'll normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './major project/flower_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "train_data_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "test_data_transforms = transforms.Compose([transforms.Resize(256),transforms.CenterCrop(224),transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_image_datasets = datasets.ImageFolder(train_dir, transform=train_data_transforms)\n",
    "valid_image_datasets = datasets.ImageFolder(valid_dir, transform=test_data_transforms)\n",
    "test_image_datasets = datasets.ImageFolder(test_dir, transform=test_data_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "train_dataloaders = torch.utils.data.DataLoader(train_image_datasets, batch_size=128)\n",
    "valid_dataloaders = torch.utils.data.DataLoader(valid_image_datasets, batch_size=128)\n",
    "test_dataloaders = torch.utils.data.DataLoader(test_image_datasets, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_iter = iter(train_dataloaders)\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "for ii in range(4):\n",
    "    ax = axes[ii]\n",
    "    helper.imshow(images[ii], ax=ax)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label mapping\n",
    "\n",
    "We have loaded in a mapping from category label to category name. You can find this in the file `cat_to_name.json` which has been imported. It's a JSON object which you can read in with the `json` module . This will give a dictionary mapping the integer encoded categories to the actual names of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./major project/cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the classifier\n",
    "\n",
    "Now that the data is ready, we'll build and train the classifier. We have used one of the pretrained models from `torchvision.models` to get the image features. Build and train a new feed-forward classifier using those features.\n",
    "\n",
    "The procedure is as follows:\n",
    "* Load a [pre-trained network](http://pytorch.org/docs/master/torchvision/models.html) (For example,the VGG networks work great and are straightforward to use)\n",
    "* Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set to determine the best hyperparameters\n",
    "\n",
    "When training make we have updated only the weights of the feed-forward network. Our goal is to be able to get the validation accuracy above 70%. To increase the efficiency we'll try different hyperparameters (learning rate, units in the classifier, epochs, etc) to find the best model. Save those hyperparameters to use if required for further procedures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "#!wget http://download.pytorch.org/models/vgg19-dcbb9e9d.pth\n",
    "#!mv vgg19-dcbb9e9d.pth ~/.torch/models/\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Building the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(25088, 10000)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('dropout1', nn.Dropout(0.3)),\n",
    "                          ('fc2', nn.Linear(10000, 4096)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('dropout2', nn.Dropout(0.3)),\n",
    "                          ('fc3', nn.Linear(4096, 1024 )),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc4', nn.Linear(1024, 102)),   \n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training my network by defining the criterion and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validation(model, valid_dataloaders, criterion):\n",
    "    model.to('cuda')\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in valid_dataloaders:\n",
    "\n",
    "        #images.resize_(images.shape[0],3, 224, 224)\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.cuda.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy\n",
    "#print(images.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from workspace_utils import active_session\n",
    "\n",
    "epochs = 25\n",
    "print_every = 40\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "    #change to cuda\n",
    "model.to('cuda')\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    for ii, (images, labels) in enumerate(train_dataloaders):\n",
    "        steps += 1\n",
    "\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "                #images.resize_(images.size()[0],3,224,224)#flattening the images\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model.forward(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "\n",
    "            model.eval()# Make sure network is in eval mode for inference\n",
    "\n",
    "            with torch.no_grad(): # Turn off gradients for validation, saves memory and computations\n",
    "                test_loss, accuracy = validation(model, valid_dataloaders, criterion)\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(valid_dataloaders)),\n",
    "                  \"Test Accuracy: {:.3f}\".format((accuracy/len(valid_dataloaders))*100))\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "                    # Make sure training is back on\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your network\n",
    "\n",
    "We'll test our trained network on test data, images the network has never seen either in training or validation. This will give a good estimate for the model's performance on completely new images. Run the test images through the network and measure the accuracy, the same way to be dione for validation. A minimum of 70% accuracy on the test set is to be gained if the model has been trained well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "model.to('cuda')\n",
    "with torch.no_grad():\n",
    "    for images,labels in test_dataloaders:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(images).to('cuda')\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the checkpoint\n",
    "\n",
    "After the network is trained, save the model so we can load it later for making predictions. We will also save the mapping of classes to indices which you get from one of the image datasets: `image_datasets['train'].class_to_idx`. You can attach this to the model as an attribute which makes inference easier later on.\n",
    "\n",
    "```model.class_to_idx = image_datasets['train'].class_to_idx```\n",
    "\n",
    "We have to completely rebuild the model later so to use it for inference. We have made sure to include any information needed in the checkpoint. If we want to load the model and keep training, we have to save the number of epochs as well as the optimizer state, `optimizer.state_dict`. We can use this trained model in the future for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint\n",
    "#Changing after the review\n",
    "model.class_to_idx = train_image_datasets.class_to_idx\n",
    "checkpoint = {'input_size':25088,\n",
    "              'output_size':102,\n",
    "              'arch':'vgg16',\n",
    "              'learning_rate':0.00001,\n",
    "              'classifier':classifier,\n",
    "              'state_dict': model.state_dict(),\n",
    "              'class_idx': model.class_to_idx,\n",
    "              #'optimizer_state':optimizer.state_dict,\n",
    "              'epochs':15,\n",
    "              }\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that loads a checkpoint and rebuilds the model\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    if checkpoint['arch'] == 'vgg16':\n",
    "        model = models.vgg16(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    learn_rate= checkpoint['learning_rate']\n",
    "    epochs = checkpoint['epochs']\n",
    "    \n",
    "    model.classifier = checkpoint['classifier']\n",
    "    \n",
    "    model.class_to_idx = checkpoint['class_idx']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for classification\n",
    "\n",
    "Here, I have defined a function which uses a trained network for inference. That is, I'll pass an image into the network and predict the class of the flower in the image. The function called `predict` will take an image and a model, then will return the top $K$ most likely classes along with the probabilities. It should look like \n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```\n",
    "\n",
    "But before that, we have to handle processing the input image such that it can be used in your network. \n",
    "\n",
    "## Image Preprocessing\n",
    "\n",
    "I'll use `PIL` to load the image. This function preprocesses the image so it can be used as input for the model. This function should process the images in the same manner used for training. \n",
    "\n",
    "First, resize the images where the shortest side is 256 pixels, keeping the aspect ratio. This can be done with the `thumbnail` or 'resize' methods. After that we'll crop out the center 224x224 portion of the image.\n",
    "\n",
    "Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1. We'll convert the values. It's easiest with a Numpy array, which we can get from a PIL image like this `np_image = np.array(pil_image)`.\n",
    "\n",
    "As before, the network expects the images to be normalized in a specific way. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`. You'll want to subtract the means from each color channel, then divide by the standard deviation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    img = Image.open(image)\n",
    "    image_process = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                      [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    np_image = image_process(img)\n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Process a PIL image for use in a PyTorch model\n",
    "image_path = (data_dir + '/test' + '/11/' + 'image_03115.jpg')\n",
    "img = process_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cross_check `process_image` function, the `imshow` function is defined which converts a PyTorch tensor and displays it in the notebook. If `process_image` function works, running the output through this function should return the original image (except for the cropped out portions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = (\"../input/flower-species/flower_data/flower_data/test/11/image_03130.jpg\")\n",
    "process_image(image_path)\n",
    "imshow(process_image(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Prediction\n",
    "\n",
    "Once we get images in the correct format, we define a function for making predictions with the trained model. A common practice is to predict the top 5 or so (usually called top-$K$) most probable classes. We'll calculate the class probabilities then find the $K$ largest values.\n",
    "\n",
    "To get the top $K$ largest values in a tensor use `x.topk(k)`. This method returns both the highest `k` probabilities and the indices of those probabilities corresponding to the classes. \n",
    "\n",
    "The we'll convert from these indices to the actual class labels using `class_to_idx` which we have added to the model used to load the data. We'll invert the dictionary to get a mapping from index to class as well.\n",
    "\n",
    "This method should take a path to an image and a model checkpoint, then return the probabilities and classes as shown here:\n",
    "\n",
    "```python\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    ''' \n",
    "    model.to('cuda')\n",
    "    image = process_image(image_path)\n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    image.unsqueeze_(0)\n",
    "    outputs = model(image.to('cuda'))\n",
    "    probs = torch.exp(outputs)\n",
    "    top_probs, top_labs = probs.topk(topk, dim=1)\n",
    "    top_probs, top_labs = top_probs.cpu(), top_labs.cpu()\n",
    "    top_probs = top_probs.detach().numpy().tolist()[0]\n",
    "    top_labs = top_labs.detach().numpy().tolist()[0]\n",
    "    idx_to_class = {val: key for key, val in model.class_to_idx.items()}\n",
    "    top_labels = [idx_to_class[lab] for lab in top_labs]\n",
    "    top_flowers = [cat_to_name[idx_to_class[lab]] for lab in top_labs]\n",
    "    return top_probs, top_labs, top_flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = (data_dir + '/test' + '/11/' + 'image_03130.jpg')\n",
    "probs, classes, flowers = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Checking\n",
    "\n",
    "Now after the trained model is ready for making predictions, we'll check to get some good results.We'll use `matplotlib` to plot the probabilities for the top 5 classes as a bar graph, along with the input image. It should look like this:\n",
    "\n",
    "We'll convert from the class integer encoding to actual flower names with the `cat_to_name.json` file. To show a PyTorch tensor as an image, we'll use the `imshow` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display an image along with the top 5 classes\n",
    "def sanity_check(image_path, model):\n",
    "    \n",
    "    plt.figure(figsize = (6,10))\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    \n",
    "    flower_num = image_path.split('/')[6]\n",
    "    flower_title_ = cat_to_name[flower_num]\n",
    "\n",
    "    image = process_image(image_path)\n",
    "    imshow(image, ax, title = flower_title_)\n",
    "    \n",
    "    probs, labs, flowers = predict(image_path, model)        #to make prediction and plot a bar graph for top 5\n",
    "                                                            \n",
    "    plt.subplot(2,1,2)\n",
    "    color_palette = sb.color_palette()[0];\n",
    "    sb.barplot(x=probs, y=flowers, color = color_palette);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check('../input/flower-species/flower_data/flower_data/test/58/image_02719.jpg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check('../input/flower-species/flower_data/flower_data/test/11/image_03130.jpg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check('../input/flower-species/flower_data/flower_data/test/74/image_01254.jpg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check('../input/flower-species/flower_data/flower_data/test/41/image_02257.jpg', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
